{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bb583c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch import nn\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from diffusers import AutoencoderKL\n",
    "from torch.nn import functional as F\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import CLIPTokenizer, CLIPTextModel\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.amp import GradScaler, autocast\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    print(f\"Seed set to {seed}\")\n",
    "\n",
    "\n",
    "set_seed()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1bdb94",
   "metadata": {},
   "source": [
    "### Xây dựng các class Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770f1b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, num_attn_heads, hidden_dim, in_proj_bias=True, out_proj_bias=True):\n",
    "        super().__init__()\n",
    "        # Số lượng head trong multi-head attention\n",
    "        self.num_heads = num_attn_heads\n",
    "        # Kích thước của mỗi head (giả sử hidden_dim chia đều cho số lượng head)\n",
    "        self.head_size = hidden_dim // num_attn_heads\n",
    "\n",
    "        # Khởi tạo một layer tuyến tính để chuyển đầu vào thành 3 vector: Q (Query), K (Key) và V (Value)\n",
    "        # Output có kích thước 3 * hidden_dim để bao gồm 3 vector này liên tiếp\n",
    "        self.qkv_proj = nn.Linear(\n",
    "            hidden_dim, 3 * hidden_dim, bias=in_proj_bias)\n",
    "\n",
    "        # Layer tuyến tính để chuyển đổi đầu ra của attention về lại kích thước hidden_dim ban đầu\n",
    "        self.output_proj = nn.Linear(\n",
    "            hidden_dim, hidden_dim, bias=out_proj_bias)\n",
    "\n",
    "    def forward(self, features, use_causal_mask=False):\n",
    "        # Lấy kích thước đầu vào: b=batch size, s=sequence length, d=feature dimension\n",
    "        b, s, d = features.shape\n",
    "\n",
    "        # Áp dụng layer qkv_proj để biến đổi đầu vào thành kết hợp của Q, K, V\n",
    "        qkv_combined = self.qkv_proj(features)\n",
    "        # Tách tensor đã kết hợp thành 3 tensor riêng biệt: Q, K, V theo chiều cuối cùng\n",
    "        q_mat, k_mat, v_mat = torch.chunk(qkv_combined, 3, dim=-1)\n",
    "\n",
    "        # Chuyển đổi kích thước của mỗi tensor Q, K, V để phù hợp với multi-head attention\n",
    "        # View lại kích thước: (batch size, sequence length, số head, kích thước head)\n",
    "        # Sau đó hoán đổi các chiều để có thứ tự: (batch size, số head, sequence length, kích thước head)\n",
    "        q_mat = q_mat.view(b, s, self.num_heads,\n",
    "                           self.head_size).permute(0, 2, 1, 3)\n",
    "        k_mat = k_mat.view(b, s, self.num_heads,\n",
    "                           self.head_size).permute(0, 2, 1, 3)\n",
    "        v_mat = v_mat.view(b, s, self.num_heads,\n",
    "                           self.head_size).permute(0, 2, 1, 3)\n",
    "\n",
    "        # Tính phép nhân ma trận giữa Q và K^T để nhận ma trận điểm (attention scores)\n",
    "        qk = torch.matmul(q_mat, k_mat.transpose(-2, -1))\n",
    "        # Chia ma trận điểm cho căn bậc hai của kích thước head để tránh giá trị quá lớn (scale factor)\n",
    "        sqrt_qk = qk / math.sqrt(self.head_size)\n",
    "\n",
    "        # Nếu bật tùy chọn sử dụng causal mask (giúp tránh rò rỉ thông tin từ tương lai),\n",
    "        # tạo một ma trận mask dạng tam giác trên và áp dụng cho scores\n",
    "        if use_causal_mask:\n",
    "            causal_mask = torch.triu(torch.ones_like(\n",
    "                sqrt_qk, dtype=torch.bool), diagonal=1)\n",
    "            sqrt_qk = sqrt_qk.masked_fill(causal_mask, -torch.inf)\n",
    "\n",
    "        # Áp dụng hàm softmax trên chiều cuối để chuẩn hóa các điểm thành xác suất\n",
    "        attn_weights = torch.softmax(sqrt_qk, dim=-1)\n",
    "        # Tính giá trị attention bằng cách nhân trọng số với V (giá trị value)\n",
    "        attn_values = torch.matmul(attn_weights, v_mat)\n",
    "\n",
    "        # Đảo lại thứ tự các chiều về như ban đầu sau khi kết hợp các head:\n",
    "        # Đổi chiều từ (batch size, số head, sequence length, kích thước head)\n",
    "        # về (batch size, sequence length, hidden_dim)\n",
    "        attn_values = attn_values.permute(\n",
    "            0, 2, 1, 3).contiguous().view(b, s, d)\n",
    "\n",
    "        # Áp dụng layer output_proj để chuyển đổi kết quả attention về lại định dạng và kích thước ban đầu của model\n",
    "        final_output = self.output_proj(attn_values)\n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c087b647",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, num_attn_heads, query_dim, context_dim, in_proj_bias=True, out_proj_bias=True):\n",
    "        super().__init__()\n",
    "        # Số lượng head trong cơ chế attention (multi-head attention)\n",
    "        self.num_heads = num_attn_heads\n",
    "        # Kích thước mỗi head, giả sử query_dim chia đều cho số lượng head\n",
    "        self.head_size = query_dim // num_attn_heads\n",
    "\n",
    "        # Ánh xạ tuyến tính để chuyển đổi đầu vào query về không gian cần thiết\n",
    "        self.query_map = nn.Linear(query_dim, query_dim, bias=in_proj_bias)\n",
    "        # Ánh xạ tuyến tính để chuyển đổi context input thành key, để phù hợp với query\n",
    "        self.key_map = nn.Linear(context_dim, query_dim, bias=in_proj_bias)\n",
    "        # Ánh xạ tuyến tính để chuyển đổi context input thành value, để phù hợp với query\n",
    "        self.value_map = nn.Linear(context_dim, query_dim, bias=in_proj_bias)\n",
    "\n",
    "        # Ánh xạ tuyến tính cuối cùng, đưa kết quả attention trở lại không gian của query ban đầu\n",
    "        self.output_map = nn.Linear(query_dim, query_dim, bias=out_proj_bias)\n",
    "\n",
    "    def forward(self, query_input, context_input):\n",
    "        # Lấy kích thước của input query: b_q=batch size, s_q=sequence length của query, d_q=dimension của query\n",
    "        b_q, s_q, d_q = query_input.shape\n",
    "        # Lấy sequence length của context input (s_kv) - các tensor key và value có cùng kích thước\n",
    "        _, s_kv, _ = context_input.shape\n",
    "\n",
    "        # Áp dụng các ánh xạ tuyến tính riêng cho query, key và value\n",
    "        q_mat = self.query_map(query_input)\n",
    "        k_mat = self.key_map(context_input)\n",
    "        v_mat = self.value_map(context_input)\n",
    "\n",
    "        # Định hình lại tensor và chuyển đổi chiều để phù hợp với cấu trúc multi-head attention\n",
    "        # Chuyển q_mat từ (batch, s_q, query_dim) về (batch, s_q, num_heads, head_size) rồi hoán đổi thành (batch, num_heads, s_q, head_size)\n",
    "        q_mat = q_mat.view(b_q, s_q, self.num_heads,\n",
    "                           self.head_size).permute(0, 2, 1, 3)\n",
    "        # Đối với key và value, sử dụng kích thước sequence từ context input (s_kv)\n",
    "        k_mat = k_mat.view(b_q, s_kv, self.num_heads,\n",
    "                           self.head_size).permute(0, 2, 1, 3)\n",
    "        v_mat = v_mat.view(b_q, s_kv, self.num_heads,\n",
    "                           self.head_size).permute(0, 2, 1, 3)\n",
    "\n",
    "        # Tính toán điểm attention: nhân Q với transpose của K\n",
    "        qk = torch.matmul(q_mat, k_mat.transpose(-2, -1))\n",
    "        # Áp dụng scale factor bằng cách chia cho căn bậc hai của head_size để ổn định giá trị\n",
    "        sqrt_qk = qk / math.sqrt(self.head_size)\n",
    "        # Áp dụng softmax trên chiều cuối của tensor để chuyển điểm thành xác suất\n",
    "        attn_weights = torch.softmax(sqrt_qk, dim=-1)\n",
    "\n",
    "        # Tính các giá trị attention bằng cách nhân trọng số (attn_weights) với tensor value (v_mat)\n",
    "        attn_values = torch.matmul(attn_weights, v_mat)\n",
    "        # Kết hợp lại các head: đổi chiều từ (batch, num_heads, s_q, head_size) về (batch, s_q, num_heads, head_size)\n",
    "        # rồi reshape về (batch, s_q, query_dim)\n",
    "        attn_values = attn_values.permute(\n",
    "            0, 2, 1, 3).contiguous().view(b_q, s_q, d_q)\n",
    "\n",
    "        # Áp dụng ánh xạ tuyến tính cuối cùng để đưa đầu ra của cross attention về không gian của query ban đầu\n",
    "        final_output = self.output_map(attn_values)\n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc458c3",
   "metadata": {},
   "source": [
    "## Denoising Diffusion Probabilistic Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46b1854",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPMScheduler:\n",
    "    def __init__(self, random_generator, train_timesteps=1000, diffusion_beta_start=0.00085, diffusion_beta_end=0.012):\n",
    "        \"\"\"\n",
    "        Khởi tạo scheduler cho quá trình diffusion với các thông số:\n",
    "         - random_generator: bộ tạo số ngẫu nhiên (PRNG) dùng cho quá trình thêm nhiễu.\n",
    "         - train_timesteps: tổng số bước huấn luyện.\n",
    "         - diffusion_beta_start, diffusion_beta_end: giá trị beta khởi đầu và kết thúc cho quá trình diffusion.\n",
    "         \n",
    "        Các bước tính toán chính:\n",
    "         1. Tạo vector betas dạng tuyến tính sau khi lấy căn bậc hai (để ổn định phân bố) rồi bình phương lại.\n",
    "         2. Tính các giá trị alphas = 1 - beta.\n",
    "         3. Tính tích lũy (cumulative product) của alphas, dùng để ước lượng mức độ tín hiệu qua các bước.\n",
    "         4. Lưu lại các giá trị và thiết lập lịch trình cho các bước diffusion.\n",
    "        \"\"\"\n",
    "        # Tạo vector betas qua linspace trên khoảng [sqrt(diffusion_beta_start), sqrt(diffusion_beta_end)]\n",
    "        # sau đó bình phương lại để đảm bảo beta có phân bố mong muốn\n",
    "        self.betas = torch.linspace(\n",
    "            diffusion_beta_start ** 0.5, diffusion_beta_end ** 0.5, train_timesteps, dtype=torch.float32) ** 2\n",
    "\n",
    "        # Tính alphas = 1 - betas\n",
    "        self.alphas = 1.0 - self.betas\n",
    "        # Tính tích lũy của alphas theo chiều 0 (mỗi bước nhân với bước trước đó)\n",
    "        self.alphas_cumulative_product = torch.cumprod(self.alphas, dim=0)\n",
    "        # Một tensor chứa giá trị 1 để dùng trong các trường hợp biên (ví dụ: bước đầu tiên)\n",
    "        self.one_val = torch.tensor(1.0)\n",
    "        # Lưu lại bộ sinh số ngẫu nhiên cho việc thêm nhiễu\n",
    "        self.prng_generator = random_generator\n",
    "        # Lưu tổng số bước huấn luyện\n",
    "        self.total_train_timesteps = train_timesteps\n",
    "        # Tạo lịch trình thời gian ban đầu (trả về tensor với các timestep giảm dần từ train_timesteps-1 đến 0)\n",
    "        self.schedule_timesteps = torch.from_numpy(\n",
    "            np.arange(0, train_timesteps)[::-1].copy())\n",
    "\n",
    "    def set_steps(self, num_sampling_steps=50):\n",
    "        \"\"\"\n",
    "        Điều chỉnh lịch trình các bước mẫu (sampling steps) dựa trên số bước mong muốn.\n",
    "         - num_sampling_steps: số bước mẫu trong quá trình sinh mẫu.\n",
    "         \n",
    "        Quá trình:\n",
    "         1. Tính hệ số chia bước (step_scaling_factor) dựa trên tổng số bước huấn luyện chia cho số bước mẫu.\n",
    "         2. Tạo mảng các timestep cho quá trình sinh mẫu theo tỉ lệ, sau đó đảo ngược thứ tự (giảm dần).\n",
    "        \"\"\"\n",
    "        self.num_sampling_steps = num_sampling_steps\n",
    "        step_scaling_factor = self.total_train_timesteps // self.num_sampling_steps\n",
    "        timesteps_for_sampling = (np.arange(\n",
    "            0, num_sampling_steps) * step_scaling_factor).round()[::-1].copy().astype(np.int64)\n",
    "        self.schedule_timesteps = torch.from_numpy(timesteps_for_sampling)\n",
    "\n",
    "    def _get_prior_timestep(self, current_timestep):\n",
    "        \"\"\"\n",
    "        Tính toán timestep trước đó trong quá trình sampling.\n",
    "         - current_timestep: timestep hiện tại.\n",
    "         \n",
    "        Công thức:\n",
    "         previous_t = current_timestep - (tổng số bước / số bước mẫu)\n",
    "        \"\"\"\n",
    "        previous_t = current_timestep - self.total_train_timesteps // self.num_sampling_steps\n",
    "        return previous_t\n",
    "\n",
    "    def _calculate_variance(self, timestep):\n",
    "        \"\"\"\n",
    "        Tính phương sai (variance) cho bước timestep hiện tại.\n",
    "         - Sử dụng tích lũy của alphas tại bước hiện tại và bước trước đó để tính toán beta hiện tại.\n",
    "         - Công thức:\n",
    "              beta_t_current = 1 - (alpha_cumprod_t / alpha_cumprod_t_prev)\n",
    "              variance = ((1 - alpha_cumprod_t_prev) / (1 - alpha_cumprod_t)) * beta_t_current\n",
    "         - Dùng torch.clamp để đảm bảo giá trị không quá nhỏ.\n",
    "        \"\"\"\n",
    "        prev_t = self._get_prior_timestep(timestep)\n",
    "        alpha_cumprod_t = self.alphas_cumulative_product[timestep]\n",
    "        # Nếu prev_t < 0 (bước đầu tiên) thì gán alpha_cumprod_t_prev bằng 1\n",
    "        alpha_cumprod_t_prev = self.alphas_cumulative_product[prev_t] if prev_t >= 0 else self.one_val\n",
    "        beta_t_current = 1 - alpha_cumprod_t / alpha_cumprod_t_prev\n",
    "        variance_value = (1 - alpha_cumprod_t_prev) / \\\n",
    "            (1 - alpha_cumprod_t) * beta_t_current\n",
    "        variance_value = torch.clamp(variance_value, min=1e-20)\n",
    "        return variance_value\n",
    "\n",
    "    def adjust_strength(self, strength_level=1):\n",
    "        \"\"\"\n",
    "        Điều chỉnh \"mức độ mạnh\" (strength) của quá trình mẫu. Điều này giúp kiểm soát mức độ thêm nhiễu ban đầu.\n",
    "         - strength_level: hệ số điều chỉnh (giá trị từ 0 đến 1), xác định bước bắt đầu của quá trình sampling.\n",
    "         \n",
    "        Quá trình:\n",
    "         1. Tính chỉ số bước bắt đầu dựa trên strength_level.\n",
    "         2. Cập nhật schedule_timesteps để chỉ lấy các bước từ chỉ số này trở đi.\n",
    "        \"\"\"\n",
    "        initial_step_index = self.num_sampling_steps - \\\n",
    "            int(self.num_sampling_steps * strength_level)\n",
    "        self.schedule_timesteps = self.schedule_timesteps[initial_step_index:]\n",
    "        # Lưu lại bước bắt đầu của quá trình sampling\n",
    "        self.start_sampling_step = initial_step_index\n",
    "\n",
    "    def step(self, current_t, current_latents, model_prediction):\n",
    "        \"\"\"\n",
    "        Thực hiện một bước ngược (denoising step) trong quá trình diffusion, sử dụng dự đoán của mô hình.\n",
    "         - current_t: timestep hiện tại.\n",
    "         - current_latents: tensor hiện tại của latent, chứa thông tin nhiễu.\n",
    "         - model_prediction: dự đoán của mô hình về nhiễu cần loại bỏ.\n",
    "         \n",
    "        Các bước chính:\n",
    "         1. Tính các giá trị tích lũy alphas (cả ở thời điểm hiện tại và bước trước đó).\n",
    "         2. Tính các hệ số alpha, beta hiện tại.\n",
    "         3. Ước tính giá trị gốc (predicted original) từ tensor latent hiện tại và dự đoán mô hình.\n",
    "         4. Tính trung bình dự đoán của bước trước đó (predicted prior mean) bằng cách kết hợp giá trị ước tính gốc và latent hiện tại với các hệ số đã tính.\n",
    "         5. Nếu t > 0, thêm một thành phần nhiễu (variance term) dựa trên phương sai tính toán của bước đó.\n",
    "         6. Trả về mẫu latent dự đoán cho bước trước đó.\n",
    "        \"\"\"\n",
    "        t = current_t\n",
    "        prev_t = self._get_prior_timestep(t)\n",
    "\n",
    "        # Lấy giá trị cumulative product của alphas tại thời điểm hiện tại và trước đó\n",
    "        alpha_cumprod_t = self.alphas_cumulative_product[t]\n",
    "        alpha_cumprod_t_prev = self.alphas_cumulative_product[prev_t] if prev_t >= 0 else self.one_val\n",
    "\n",
    "        # Tính beta cumulative cho thời điểm hiện tại và trước đó\n",
    "        beta_cumprod_t = 1 - alpha_cumprod_t\n",
    "        beta_cumprod_t_prev = 1 - alpha_cumprod_t_prev\n",
    "\n",
    "        # Tính hệ số chuyển đổi giữa hai bước\n",
    "        alpha_t_current = alpha_cumprod_t / alpha_cumprod_t_prev\n",
    "        beta_t_current = 1 - alpha_t_current\n",
    "\n",
    "        # Dự đoán mẫu gốc ban đầu từ latent hiện tại và dự đoán của mô hình\n",
    "        predicted_original = (current_latents - beta_cumprod_t **\n",
    "                              0.5 * model_prediction) / alpha_cumprod_t ** 0.5\n",
    "\n",
    "        # Tính các hệ số trọng số để kết hợp dự đoán gốc và latent hiện tại\n",
    "        original_coeff = (alpha_cumprod_t_prev ** 0.5 *\n",
    "                          beta_t_current) / beta_cumprod_t\n",
    "        current_coeff = alpha_t_current ** 0.5 * beta_cumprod_t_prev / beta_cumprod_t\n",
    "\n",
    "        # Tính trung bình của bước trước đó (predicted prior mean)\n",
    "        predicted_prior_mean = original_coeff * \\\n",
    "            predicted_original + current_coeff * current_latents\n",
    "\n",
    "        # Khởi tạo thành phần nhiễu cho bước hiện tại\n",
    "        variance_term = 0\n",
    "        if t > 0:\n",
    "            target_device = model_prediction.device\n",
    "            # Sinh nhiễu chuẩn theo hình dạng của dự đoán của mô hình\n",
    "            noise_component = torch.randn(\n",
    "                model_prediction.shape, generator=self.prng_generator, device=target_device, dtype=model_prediction.dtype)\n",
    "            variance_term = (self._calculate_variance(t)\n",
    "                             ** 0.5) * noise_component\n",
    "\n",
    "        # Mẫu dự đoán cho bước trước đó = trung bình dự đoán + nhiễu (nếu có)\n",
    "        predicted_prior_sample = predicted_prior_mean + variance_term\n",
    "        return predicted_prior_sample\n",
    "\n",
    "    def add_noise(self, initial_samples, noise_timesteps):\n",
    "        \"\"\"\n",
    "        Thêm nhiễu vào samples ban đầu dựa trên các noise timestep đã cho.\n",
    "         - initial_samples: tensor mẫu ban đầu (latent representation).\n",
    "         - noise_timesteps: các timestep mà tại đó sẽ thêm nhiễu.\n",
    "         \n",
    "        Quá trình:\n",
    "         1. Chuyển alphas_cumulative_product về cùng thiết bị và kiểu dữ liệu với initial_samples.\n",
    "         2. Tính căn bậc hai của alphas_cumprod và (1 - alphas_cumprod) cho các timestep được chọn.\n",
    "         3. Sinh nhiễu chuẩn có cùng hình dạng với initial_samples.\n",
    "         4. Tính mẫu nhiễu (noisy_result) dựa trên công thức kết hợp giữa initial_samples và nhiễu.\n",
    "         5. Trả về cả noisy_result và random_noise (nhiễu đã thêm vào).\n",
    "        \"\"\"\n",
    "        # Đảm bảo alphas_cumulative_product có cùng device và dtype với initial_samples\n",
    "        alphas_cumprod = self.alphas_cumulative_product.to(\n",
    "            device=initial_samples.device, dtype=initial_samples.dtype)\n",
    "        noise_timesteps = noise_timesteps.to(initial_samples.device)\n",
    "\n",
    "        # Tính căn bậc hai của alphas_cumprod tại các noise timestep\n",
    "        sqrt_alpha_cumprod = alphas_cumprod[noise_timesteps] ** 0.5\n",
    "        sqrt_alpha_cumprod = sqrt_alpha_cumprod.view(\n",
    "            sqrt_alpha_cumprod.shape[0], *([1] * (initial_samples.ndim - 1)))\n",
    "\n",
    "        # Tính căn bậc hai của (1 - alphas_cumprod) tại các noise timestep\n",
    "        sqrt_one_minus_alpha_cumprod = (\n",
    "            1 - alphas_cumprod[noise_timesteps]) ** 0.5\n",
    "        sqrt_one_minus_alpha_cumprod = sqrt_one_minus_alpha_cumprod.view(\n",
    "            sqrt_one_minus_alpha_cumprod.shape[0], *([1] * (initial_samples.ndim - 1)))\n",
    "\n",
    "        # Sinh nhiễu chuẩn với hình dạng của initial_samples\n",
    "        random_noise = torch.randn(initial_samples.shape, generator=self.prng_generator,\n",
    "                                   device=initial_samples.device, dtype=initial_samples.dtype)\n",
    "        # Tạo mẫu nhiễu bằng cách kết hợp initial_samples và nhiễu, sử dụng các hệ số tỷ lệ vừa tính\n",
    "        noisy_result = sqrt_alpha_cumprod * initial_samples + \\\n",
    "            sqrt_one_minus_alpha_cumprod * random_noise\n",
    "        return noisy_result, random_noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f06eb3",
   "metadata": {},
   "source": [
    "## kiến trúc U-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef513f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNET_ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, time_dim=1280):\n",
    "        \"\"\"\n",
    "        Khởi tạo residual block cho UNet, tích hợp thông tin từ các đặc trưng ảnh và embedding của thời gian.\n",
    "        \n",
    "        Arguments:\n",
    "         - in_channels: số kênh đầu vào.\n",
    "         - out_channels: số kênh đầu ra.\n",
    "         - time_dim: kích thước của vector embedding thời gian (mặc định là 1280).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Áp dụng Group Normalization lên đầu vào (chia nhỏ theo từng nhóm gồm 32 channel)\n",
    "        self.gn_feature = nn.GroupNorm(32, in_channels)\n",
    "        # Lớp tích chập với kernel 3x3 (padding=1 để giữ nguyên kích thước) xử lý đặc trưng ảnh\n",
    "        self.conv_feature = nn.Conv2d(\n",
    "            in_channels, out_channels, kernel_size=3, padding=1)\n",
    "\n",
    "        # Lớp Linear chuyển đổi embedding thời gian về số chiều tương ứng với out_channels,\n",
    "        # giúp tích hợp thông tin thời gian vào các đặc trưng ảnh\n",
    "        self.time_embedding_proj = nn.Linear(time_dim, out_channels)\n",
    "\n",
    "        # Sau khi tích hợp thông tin, dùng Group Normalization để ổn định giá trị kích hoạt\n",
    "        self.gn_merged = nn.GroupNorm(32, out_channels)\n",
    "        # Lớp tích chập thứ hai với kernel 3x3, giúp trộn lẫn đặc trưng ảnh đã kết hợp với embedding thời gian\n",
    "        self.conv_merged = nn.Conv2d(\n",
    "            out_channels, out_channels, kernel_size=3, padding=1)\n",
    "\n",
    "        # Residual connection: nếu số kênh đầu vào bằng số kênh đầu ra thì dùng Identity,\n",
    "        # ngược lại thì sử dụng một lớp conv 1x1 để chuyển đổi số kênh phù hợp với đầu ra\n",
    "        if in_channels == out_channels:\n",
    "            self.residual_connection = nn.Identity()\n",
    "        else:\n",
    "            self.residual_connection = nn.Conv2d(\n",
    "                in_channels, out_channels, kernel_size=1, padding=0)\n",
    "\n",
    "    def forward(self, input_feature, time_emb):\n",
    "        \"\"\"\n",
    "        Quy trình xử lý một forward pass của block.\n",
    "        \n",
    "        Arguments:\n",
    "         - input_feature: tensor đặc trưng đầu vào có shape (batch, in_channels, height, width)\n",
    "         - time_emb: vector embedding thời gian, có shape (batch, time_dim)\n",
    "         \n",
    "        Quá trình:\n",
    "         1. Áp dụng Group Norm và kích hoạt Silu cho input_feature.\n",
    "         2. Áp dụng convolution đầu tiên để tạo ra tensor đặc trưng tạm thời.\n",
    "         3. Xử lý time_emb qua kích hoạt Silu và lớp Linear để đưa về kích thước tương ứng với out_channels.\n",
    "         4. Mở rộng chiều của time_emb để có thể cộng với tensor từ convolution.\n",
    "         5. Cộng embedding thời gian vào đặc trưng tạm thời và tiếp tục xử lý qua Group Norm, kích hoạt Silu và convolution thứ hai.\n",
    "         6. Cuối cùng, cộng kết quả với residual connection để cho kết quả cuối cùng.\n",
    "        \"\"\"\n",
    "        # Lưu lại tensor gốc để thực hiện skip-connection\n",
    "        residual = input_feature\n",
    "\n",
    "        # Bước 1: Chuẩn hóa các đặc trưng đầu vào và áp dụng kích hoạt Silu\n",
    "        h = self.gn_feature(input_feature)\n",
    "        h = F.silu(h)\n",
    "        # Bước 2: Áp dụng convolution đầu tiên để lấy các đặc trưng mới\n",
    "        h = self.conv_feature(h)\n",
    "\n",
    "        # Bước 3: Xử lý embedding thời gian, áp dụng kích hoạt Silu trước khi chuyển qua lớp Linear\n",
    "        time_emb_processed = F.silu(time_emb)\n",
    "        time_emb_projected = self.time_embedding_proj(time_emb_processed)\n",
    "        # Bước 4: Mở rộng chiều của time_emb_projected để có thể cộng với tensor h (với shape tương ứng)\n",
    "        time_emb_projected = time_emb_projected.unsqueeze(-1).unsqueeze(-1)\n",
    "\n",
    "        # Bước 5: Cộng thông tin embedding thời gian vào tensor đặc trưng, rồi chuẩn hóa và xử lý qua convolution thứ hai\n",
    "        merged_feature = h + time_emb_projected\n",
    "        merged_feature = self.gn_merged(merged_feature)\n",
    "        merged_feature = F.silu(merged_feature)\n",
    "        merged_feature = self.conv_merged(merged_feature)\n",
    "\n",
    "        # Bước 6: Thực hiện skip-connection: cộng kết quả sau xử lý với tensor ban đầu (hoặc được chuyển đổi qua conv 1x1 nếu cần)\n",
    "        output = merged_feature + self.residual_connection(residual)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c627fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNET_AttentionBlock(nn.Module):\n",
    "    def __init__(self, num_heads, head_dim, context_dim=512):\n",
    "        \"\"\"\n",
    "        Khởi tạo Attention Block trong UNet, tích hợp cả Self-Attention và Cross-Attention,\n",
    "        và một Feed-Forward Network (FFN) sử dụng cơ chế GEGLU.\n",
    "\n",
    "        Parameters:\n",
    "         - num_heads: số lượng head trong attention.\n",
    "         - head_dim: kích thước của mỗi head.\n",
    "         - context_dim: chiều của context (dữ liệu bối cảnh) dùng cho Cross-Attention (mặc định 512).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Tính tổng chiều embedding: số head nhân với kích thước mỗi head.\n",
    "        embed_dim = num_heads * head_dim\n",
    "\n",
    "        # Chuẩn hóa đầu vào với GroupNorm giúp ổn định giá trị kích hoạt.\n",
    "        self.gn_in = nn.GroupNorm(32, embed_dim, eps=1e-6)\n",
    "        # Dự án đầu vào qua Conv2d với kernel 1x1, giữ nguyên kích thước spatial.\n",
    "        self.proj_in = nn.Conv2d(embed_dim, embed_dim,\n",
    "                                 kernel_size=1, padding=0)\n",
    "\n",
    "        # -------- Self-Attention -----------\n",
    "        # LayerNorm trước attention giúp chuẩn hóa tensor theo chiều cuối.\n",
    "        self.ln_1 = nn.LayerNorm(embed_dim)\n",
    "        # Self-Attention: cho phép mỗi vị trí trong chuỗi \"chú ý\" tới các vị trí khác cùng chuỗi.\n",
    "        self.attn_1 = SelfAttention(num_heads, embed_dim, in_proj_bias=False)\n",
    "\n",
    "        # -------- Cross-Attention -----------\n",
    "        # LayerNorm cho phần cross-attention.\n",
    "        self.ln_2 = nn.LayerNorm(embed_dim)\n",
    "        # Cross-Attention: cho phép kết hợp thông tin từ input chính và context bên ngoài (ví dụ như embedding hướng dẫn).\n",
    "        self.attn_2 = CrossAttention(\n",
    "            num_heads, embed_dim, context_dim, in_proj_bias=False)\n",
    "\n",
    "        # LayerNorm trước phần FFN.\n",
    "        self.ln_3 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        # -------- Feed-Forward Network (FFN) với GEGLU -----------\n",
    "        # FFN sử dụng cơ chế GEGLU: đầu ra của layer Linear được chia làm 2 phần (intermediate và gate)\n",
    "        # Kích thước output của lớp này là 4 * embed_dim * 2 vì sẽ chia làm 2 phần sau.\n",
    "        self.ffn_geglu = nn.Linear(embed_dim, 4 * embed_dim * 2)\n",
    "        # Lớp Linear sau GEGLU giúp giảm chiều về embed_dim.\n",
    "        self.ffn_out = nn.Linear(4 * embed_dim, embed_dim)\n",
    "        # Dự án đầu ra qua Conv2d với kernel 1x1 để chuyển đổi không gian đặc trưng về dạng ban đầu.\n",
    "        self.proj_out = nn.Conv2d(\n",
    "            embed_dim, embed_dim, kernel_size=1, padding=0)\n",
    "\n",
    "    def forward(self, input_tensor, context_tensor):\n",
    "        \"\"\"\n",
    "        Forward pass của Attention Block.\n",
    "\n",
    "        Parameters:\n",
    "         - input_tensor: tensor đầu vào với shape (B, C, H, W) (B=batch, C=channels, H/W=chiều cao/rộng).\n",
    "         - context_tensor: tensor bối cảnh dùng cho Cross-Attention.\n",
    "        \n",
    "        Quy trình xử lý:\n",
    "         1. Dự án và reshape input để chuyển về dạng chuỗi (flatten spatial dimensions).\n",
    "         2. Áp dụng Self-Attention kèm skip connection.\n",
    "         3. Áp dụng Cross-Attention với thông tin context, kèm skip connection.\n",
    "         4. Áp dụng FFN với GEGLU và kết nối skip.\n",
    "         5. Chuyển tensor về lại shape ban đầu và dự án đầu ra.\n",
    "        \"\"\"\n",
    "        # Lưu lại input gốc để dùng cho skip connection cuối cùng.\n",
    "        skip_connection = input_tensor\n",
    "\n",
    "        # Lấy kích thước của input: B=batch, C=channels, H=height, W=width.\n",
    "        B, C, H, W = input_tensor.shape\n",
    "        HW = H * W  # Tính tổng số điểm ảnh (flatten không gian)\n",
    "\n",
    "        # --- Pre-processing ---\n",
    "        # Áp dụng GroupNorm và dự án đầu vào (Conv2d) để chuẩn bị dữ liệu.\n",
    "        h = self.gn_in(input_tensor)\n",
    "        h = self.proj_in(h)\n",
    "        # Reshape từ (B, C, H, W) về (B, C, HW) và transpose để có dạng (B, HW, C),\n",
    "        # thuận tiện cho việc áp dụng LayerNorm và attention.\n",
    "        h = h.view(B, C, HW).transpose(-1, -2)\n",
    "\n",
    "        # --- Self-Attention ---\n",
    "        # Lưu kết quả trước khi attention để dùng cho skip connection.\n",
    "        attn1_skip = h.clone()\n",
    "        h = self.ln_1(h)\n",
    "        h = self.attn_1(h)\n",
    "        # Cộng skip connection sau Self-Attention.\n",
    "        h = h + attn1_skip\n",
    "\n",
    "        # --- Cross-Attention ---\n",
    "        attn2_skip = h.clone()\n",
    "        h = self.ln_2(h)\n",
    "        h = self.attn_2(h, context_tensor)\n",
    "        h = h + attn2_skip\n",
    "\n",
    "        # --- Feed-Forward Network (FFN) với GEGLU ---\n",
    "        ffn_skip = h.clone()\n",
    "        h = self.ln_3(h)\n",
    "        # Áp dụng lớp Linear và chia thành 2 phần: intermediate và gate.\n",
    "        intermediate, gate = self.ffn_geglu(h).chunk(2, dim=-1)\n",
    "        # Sử dụng GEGLU: nhân phần intermediate với gelu của gate.\n",
    "        h = intermediate * F.gelu(gate)\n",
    "        h = self.ffn_out(h)\n",
    "        h = h + ffn_skip  # Skip connection cho FFN\n",
    "\n",
    "        # --- Post-processing ---\n",
    "        # Chuyển tensor về lại shape ban đầu: (B, HW, C) -> (B, C, H, W)\n",
    "        h = h.transpose(-1, -2).view(B, C, H, W)\n",
    "        # Dự án đầu ra qua Conv2d và cộng với skip connection ban đầu.\n",
    "        output = self.proj_out(h) + skip_connection\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a611beec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwitchSequential(nn.Sequential):\n",
    "    def forward(self, x, guidance_context, time_embedding):\n",
    "        \"\"\"\n",
    "        Thực hiện forward pass qua các module nằm trong SwitchSequential.\n",
    "        \n",
    "        Quy trình xử lý:\n",
    "         - Duyệt qua từng module con trong danh sách.\n",
    "         - Nếu module là UNET_AttentionBlock, truyền vào 2 tham số: tensor input và guidance_context.\n",
    "         - Nếu module là UNET_ResidualBlock, truyền vào 2 tham số: tensor input và time_embedding.\n",
    "         - Với các module khác, chỉ truyền input tensor x.\n",
    "         \n",
    "        Tham số:\n",
    "         - x: tensor đầu vào.\n",
    "         - guidance_context: tensor chứa thông tin bối cảnh dùng cho cross attention.\n",
    "         - time_embedding: vector embedding thời gian dùng cho residual block.\n",
    "        \"\"\"\n",
    "        for module_instance in self:\n",
    "            # Nếu module là UNET_AttentionBlock, thì gọi forward với guidance_context\n",
    "            if isinstance(module_instance, UNET_AttentionBlock):\n",
    "                x = module_instance(x, guidance_context)\n",
    "            # Nếu module là UNET_ResidualBlock, thì gọi forward với time_embedding\n",
    "            elif isinstance(module_instance, UNET_ResidualBlock):\n",
    "                x = module_instance(x, time_embedding)\n",
    "            # Với các module khác, chỉ truyền x\n",
    "            else:\n",
    "                x = module_instance(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class TimeEmbedding(nn.Module):\n",
    "    def __init__(self, n_embd):\n",
    "        \"\"\"\n",
    "        Khởi tạo lớp TimeEmbedding dùng để chuyển đổi embedding thời gian.\n",
    "        \n",
    "        Các tham số:\n",
    "         - n_embd: số chiều của embedding đầu vào.\n",
    "         \n",
    "        Cấu trúc:\n",
    "         - proj1: lớp Linear chuyển đổi từ n_embd về 4 * n_embd.\n",
    "         - proj2: lớp Linear chuyển đổi từ 4 * n_embd về 4 * n_embd.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.proj1 = nn.Linear(n_embd, 4 * n_embd)\n",
    "        self.proj2 = nn.Linear(4 * n_embd, 4 * n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass của TimeEmbedding:\n",
    "         1. Áp dụng lớp Linear đầu tiên (proj1).\n",
    "         2. Áp dụng hàm kích hoạt Silu (Sigmoid Linear Unit) để tăng tính phi tuyến.\n",
    "         3. Áp dụng lớp Linear thứ hai (proj2).\n",
    "         4. Trả về embedding đã được xử lý.\n",
    "         \n",
    "        Tham số:\n",
    "         - x: vector embedding thời gian đầu vào (shape: [batch_size, n_embd]).\n",
    "        \"\"\"\n",
    "        x = self.proj1(x)\n",
    "        x = F.silu(x)\n",
    "        x = self.proj2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abfa0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# DownBlock: Giảm kích thước không gian của tensor (encoder)\n",
    "# ====================================================\n",
    "class DownBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, n_head, time_dim=320, context_dim=512):\n",
    "        \"\"\"\n",
    "        Khối downsample gồm một SwitchSequential block (gồm residual block & attention block)\n",
    "        và một lớp downsampling dùng Conv2d với stride = 2.\n",
    "        \n",
    "        Tham số:\n",
    "          - in_channels: số kênh đầu vào.\n",
    "          - out_channels: số kênh đầu ra của block.\n",
    "          - n_head: số head trong attention block.\n",
    "          - time_dim: chiều của vector time embedding.\n",
    "          - context_dim: chiều của guidance context.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # SwitchSequential gồm:\n",
    "        #   + UNET_ResidualBlock: tích hợp time embedding\n",
    "        #   + UNET_AttentionBlock: tích hợp guidance context\n",
    "        self.block = SwitchSequential(\n",
    "            UNET_ResidualBlock(in_channels, out_channels, time_dim=time_dim),\n",
    "            UNET_AttentionBlock(n_head, head_dim=out_channels //\n",
    "                                n_head, context_dim=context_dim)\n",
    "        )\n",
    "        # Lớp downsample giảm kích thước spatial (stride=2)\n",
    "        self.downsample = nn.Conv2d(\n",
    "            out_channels, out_channels, kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "    def forward(self, x, guidance_context, time_embedding):\n",
    "        # Áp dụng block xử lý (residual + attention)\n",
    "        x = self.block(x, guidance_context, time_embedding)\n",
    "        # Lưu lại tensor trước khi downsample làm skip connection\n",
    "        skip = x\n",
    "        # Giảm kích thước không gian của đặc trưng\n",
    "        x = self.downsample(x)\n",
    "        return x, skip\n",
    "\n",
    "\n",
    "# ====================================================\n",
    "# UpBlock: Tăng kích thước không gian của tensor (decoder)\n",
    "# ====================================================\n",
    "class UpBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, n_head, time_dim=320, context_dim=512):\n",
    "        \"\"\"\n",
    "        Khối upsample gồm một lớp upsampling dùng ConvTranspose2d\n",
    "        và một SwitchSequential block xử lý sau khi concat skip connection.\n",
    "        \n",
    "        Tham số:\n",
    "          - in_channels: số kênh đầu vào (từ bottleneck).\n",
    "          - out_channels: số kênh đầu ra của block.\n",
    "          - n_head: số head trong attention block.\n",
    "          - time_dim: chiều của vector time embedding.\n",
    "          - context_dim: chiều của guidance context.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Lớp upsample tăng kích thước không gian (stride=2)\n",
    "        self.upsample = nn.ConvTranspose2d(\n",
    "            in_channels, out_channels, kernel_size=4, stride=2, padding=1)\n",
    "        # Sau khi upsample, ta concat skip connection -> số kênh sẽ gấp đôi (out_channels * 2)\n",
    "        # SwitchSequential block xử lý sự kết hợp đó (residual + attention)\n",
    "        self.block = SwitchSequential(\n",
    "            UNET_ResidualBlock(\n",
    "                out_channels * 2, out_channels, time_dim=time_dim),\n",
    "            UNET_AttentionBlock(n_head, head_dim=out_channels //\n",
    "                                n_head, context_dim=context_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, skip, guidance_context, time_embedding):\n",
    "        # Tăng kích thước không gian của đặc trưng\n",
    "        x = self.upsample(x)\n",
    "        # Nối (concatenate) skip connection từ encoder theo chiều kênh\n",
    "        x = torch.cat([x, skip], dim=1)\n",
    "        # Áp dụng block xử lý sau khi nối\n",
    "        x = self.block(x, guidance_context, time_embedding)\n",
    "        return x\n",
    "\n",
    "\n",
    "# ====================================================\n",
    "# UNET: Kiến trúc UNET tích hợp các module trên\n",
    "# ====================================================\n",
    "class UNET(nn.Module):\n",
    "    def __init__(self, h_dim=128, n_head=4, time_dim=320, context_dim=512):\n",
    "        \"\"\"\n",
    "        Kiến trúc UNET tổng thể, bao gồm phần encoder (downsampling),\n",
    "        bottleneck (mid block) và decoder (upsampling).\n",
    "        \n",
    "        Tham số:\n",
    "          - h_dim: số kênh cơ sở ban đầu.\n",
    "          - n_head: số head cho các attention blocks.\n",
    "          - time_dim: chiều của time embedding (đầu ra từ TimeEmbedding).\n",
    "          - context_dim: chiều của guidance context (đầu ra từ CLIPTextEncoder).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Lớp chuyển đổi đầu vào: từ latent (thường có 4 kênh) về h_dim channels.\n",
    "        self.in_conv = nn.Conv2d(4, h_dim, kernel_size=3, padding=1)\n",
    "\n",
    "        # =============================\n",
    "        # Encoder: Downsampling\n",
    "        # =============================\n",
    "        # Down block 1: chuyển từ h_dim -> h_dim * 2\n",
    "        self.down_block1 = DownBlock(\n",
    "            h_dim, h_dim * 2, n_head, time_dim=time_dim, context_dim=context_dim)\n",
    "        # Down block 2: chuyển từ h_dim * 2 -> h_dim * 4\n",
    "        self.down_block2 = DownBlock(\n",
    "            h_dim * 2, h_dim * 4, n_head, time_dim=time_dim, context_dim=context_dim)\n",
    "\n",
    "        # =============================\n",
    "        # Bottleneck (Mid Block)\n",
    "        # =============================\n",
    "        # Sử dụng SwitchSequential để xâu chuỗi các khối:\n",
    "        #   + UNET_ResidualBlock: tích hợp time embedding\n",
    "        #   + UNET_AttentionBlock: tích hợp guidance context\n",
    "        #   + UNET_ResidualBlock: tiếp tục xử lý\n",
    "        self.mid_block = SwitchSequential(\n",
    "            UNET_ResidualBlock(h_dim * 4, h_dim * 4, time_dim=time_dim),\n",
    "            UNET_AttentionBlock(n_head, head_dim=(\n",
    "                h_dim * 4) // n_head, context_dim=context_dim),\n",
    "            UNET_ResidualBlock(h_dim * 4, h_dim * 4, time_dim=time_dim)\n",
    "        )\n",
    "\n",
    "        # =============================\n",
    "        # Decoder: Upsampling\n",
    "        # =============================\n",
    "        # Up block 1: từ h_dim * 4 về h_dim * 2 (kết hợp skip connection từ down_block2)\n",
    "        self.up_block1 = UpBlock(\n",
    "            h_dim * 4, h_dim * 2, n_head, time_dim=time_dim, context_dim=context_dim)\n",
    "        # Up block 2: từ h_dim * 2 về h_dim (kết hợp skip connection từ down_block1)\n",
    "        self.up_block2 = UpBlock(\n",
    "            h_dim * 2, h_dim, n_head, time_dim=time_dim, context_dim=context_dim)\n",
    "\n",
    "        # Lớp cuối cùng: chuyển từ h_dim về số kênh cần thiết (ở diffusion model thường là 4)\n",
    "        self.out_conv = nn.Conv2d(h_dim, 4, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x, guidance_context, time_embedding):\n",
    "        \"\"\"\n",
    "        Forward pass của UNET.\n",
    "        \n",
    "        Tham số:\n",
    "          - x: tensor không gian tiềm ẩn (latent image), thường có 4 kênh.\n",
    "          - guidance_context: tensor chứa embedding của prompt (từ CLIPTextEncoder).\n",
    "          - time_embedding: embedding thời gian (từ TimeEmbedding).\n",
    "        \"\"\"\n",
    "        # 1. Chuyển đầu vào qua lớp conv ban đầu\n",
    "        x = self.in_conv(x)\n",
    "\n",
    "        # 2. Encoder: Downsample và lưu lại skip connection\n",
    "        # sau block 1, skip1 có kích thước tương ứng với h_dim*2\n",
    "        x, skip1 = self.down_block1(x, guidance_context, time_embedding)\n",
    "        # sau block 2, skip2 có kích thước tương ứng với h_dim*4\n",
    "        x, skip2 = self.down_block2(x, guidance_context, time_embedding)\n",
    "\n",
    "        # 3. Bottleneck: xử lý ở mức thấp nhất của không gian đặc trưng\n",
    "        x = self.mid_block(x, guidance_context, time_embedding)\n",
    "\n",
    "        # 4. Decoder: Upsample và kết hợp skip connections\n",
    "        x = self.up_block1(x, skip2, guidance_context, time_embedding)\n",
    "        x = self.up_block2(x, skip1, guidance_context, time_embedding)\n",
    "\n",
    "        # 5. Lớp chuyển đổi cuối cùng để tạo kết quả đầu ra\n",
    "        x = self.out_conv(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eabe755",
   "metadata": {},
   "source": [
    "## Xây dựng hàm mã hóa thông tin thời gian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a109921e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_a_timestep(timestep, embedding_dim=320):\n",
    "    \"\"\"\n",
    "    Nhúng một giá trị timestep thành vector embedding sử dụng các hàm cosine và sine.\n",
    "    \n",
    "    Các bước thực hiện:\n",
    "     1. Tính nửa số chiều embedding (half_dim) từ embedding_dim.\n",
    "     2. Tạo vector các tần số (freqs) theo cấp số nhân giảm dần, sử dụng log của 10000 để điều chỉnh tỉ lệ.\n",
    "     3. Nhân giá trị timestep với vector tần số để chuẩn bị đầu vào cho các hàm cosine và sine.\n",
    "     4. Tính toán vector cosine và sine của kết quả, sau đó nối hai vector lại theo chiều cuối.\n",
    "    \n",
    "    Tham số:\n",
    "     - timestep: giá trị timestep cần nhúng (dạng số hoặc tensor chứa một số).\n",
    "     - embedding_dim: số chiều của vector embedding (mặc định 320).\n",
    "     \n",
    "    Trả về:\n",
    "     - Một tensor chứa vector nhúng có kích thước embedding_dim.\n",
    "    \"\"\"\n",
    "    # Tính số chiều mỗi nửa của vector embedding\n",
    "    half_dim = embedding_dim // 2\n",
    "\n",
    "    # Tạo vector tần số: sử dụng hàm exp để tạo các tần số giảm dần từ 0 đến half_dim,\n",
    "    # với tỉ lệ giảm là -log(10000)/half_dim\n",
    "    freqs = torch.exp(\n",
    "        -math.log(10000) * torch.arange(start=0, end=half_dim,\n",
    "                                        dtype=torch.float32) / half_dim\n",
    "    )\n",
    "\n",
    "    # Nhân giá trị timestep (đưa về tensor) với vector freqs\n",
    "    x = torch.tensor([timestep], dtype=torch.float32)[:, None] * freqs[None]\n",
    "\n",
    "    # Tính cosine và sine cho x, sau đó nối kết quả lại theo chiều cuối (embedding_dim)\n",
    "    return torch.cat([torch.cos(x), torch.sin(x)], dim=-1)\n",
    "\n",
    "\n",
    "def embed_timesteps(timesteps, embedding_dim=320):\n",
    "    \"\"\"\n",
    "    Nhúng nhiều giá trị timesteps thành các vector embedding sử dụng hàm cosine và sine.\n",
    "    \n",
    "    Các bước thực hiện:\n",
    "     1. Xác định half_dim từ embedding_dim.\n",
    "     2. Tạo vector các tần số (freqs) giảm dần dựa trên log(10000).\n",
    "     3. Nhân mỗi giá trị timestep với vector tần số để tạo tensor nhân đôi (args).\n",
    "     4. Áp dụng hàm cosine và sine lên tensor args.\n",
    "     5. Nối các tensor cosine và sine theo chiều cuối cùng để thu được vector embedding cuối cùng.\n",
    "    \n",
    "    Tham số:\n",
    "     - timesteps: tensor chứa các giá trị timestep, có kích thước (batch_size, ) hoặc (batch_size, 1).\n",
    "     - embedding_dim: số chiều của vector embedding (mặc định 320).\n",
    "     \n",
    "    Trả về:\n",
    "     - Một tensor chứa các vector nhúng cho mỗi timestep với kích thước (batch_size, embedding_dim).\n",
    "    \"\"\"\n",
    "    # Tính số chiều của một nửa vector embedding\n",
    "    half_dim = embedding_dim // 2\n",
    "\n",
    "    # Tạo vector tần số, đảm bảo nó có cùng device với tensor timesteps\n",
    "    freqs = torch.exp(\n",
    "        -math.log(10000) * torch.arange(half_dim,\n",
    "                                        dtype=torch.float32) / half_dim\n",
    "    ).to(timesteps.device)\n",
    "\n",
    "    # Nhân mỗi giá trị timestep với vector tần số\n",
    "    args = timesteps[:, None].float() * freqs[None, :]\n",
    "\n",
    "    # Tính cosine và sine cho args, sau đó nối chúng lại theo chiều cuối\n",
    "    return torch.cat([torch.cos(args), torch.sin(args)], dim=-1)\n",
    "\n",
    "class UNETOutputLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        \"\"\"\n",
    "        Khởi tạo lớp UNETOutputLayer dùng để chuyển đổi các đặc trưng đầu ra từ UNET \n",
    "        về định dạng cuối cùng (số kênh cần thiết, thường là số kênh của latent từ VAE).\n",
    "        \n",
    "        Tham số:\n",
    "         - in_channels: số kênh của tensor đầu vào (đầu ra của decoder UNET).\n",
    "         - out_channels: số kênh của tensor đầu ra mong muốn.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Sử dụng một lớp convolution 3x3 với padding=1 để giữ nguyên kích thước không gian,\n",
    "        # giúp chuyển đổi số kênh từ in_channels sang out_channels.\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels,\n",
    "                              kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass của UNETOutputLayer.\n",
    "        \n",
    "        Tham số:\n",
    "         - x: tensor đầu vào từ UNET, có kích thước (B, in_channels, H, W).\n",
    "         \n",
    "        Trả về:\n",
    "         - Tensor đầu ra với kích thước (B, out_channels, H, W).\n",
    "        \"\"\"\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2607b716",
   "metadata": {},
   "source": [
    "## Khai báo model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed339081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------- Diffusion Model -------------------------\n",
    "class Diffusion(nn.Module):\n",
    "    def __init__(self, h_dim=128, n_head=4):\n",
    "        \"\"\"\n",
    "        Khởi tạo mô hình Diffusion.\n",
    "        \n",
    "        Tham số:\n",
    "         - h_dim: chiều ẩn của UNET.\n",
    "         - n_head: số lượng head trong attention của UNET.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Tạo embedding thời gian với kích thước 320\n",
    "        self.time_embedding = TimeEmbedding(320)\n",
    "        # UNET: mô hình chính xử lý không gian tiềm ẩn (latent), context và time embedding\n",
    "        self.unet = UNET(h_dim, n_head)\n",
    "        # Lớp chuyển đổi đầu ra của UNET về định dạng mong muốn\n",
    "        self.unet_output = UNETOutputLayer(h_dim, 4)\n",
    "\n",
    "    @torch.autocast(device_type='cuda', dtype=torch.float16, enabled=True, cache_enabled=True)\n",
    "    def forward(self, latent, context, time):\n",
    "        \"\"\"\n",
    "        Forward pass của mô hình Diffusion.\n",
    "        \n",
    "        Tham số:\n",
    "         - latent: tensor không gian tiềm ẩn (latent representation) của ảnh.\n",
    "         - context: thông tin context (ví dụ mô tả ảnh mã hóa từ CLIP).\n",
    "         - time: timestep hoặc vector thời gian.\n",
    "        \"\"\"\n",
    "        # Chuyển đổi time thông qua lớp time_embedding\n",
    "        time = self.time_embedding(time)\n",
    "        # Áp dụng UNET với latent, context và time embedding\n",
    "        output = self.unet(latent, context, time)\n",
    "        # Chuyển đổi đầu ra của UNET qua lớp unet_output để ra kết quả cuối cùng\n",
    "        output = self.unet_output(output)\n",
    "        return output\n",
    "\n",
    "# ------------------------- CLIP Text Encoder -------------------------\n",
    "class CLIPTextEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Khởi tạo mô hình CLIPTextEncoder để mã hóa các mô tả (prompts) của ảnh thành vector embedding.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Định danh của mô hình CLIP được sử dụng\n",
    "        CLIP_id = \"openai/clip-vit-base-patch32\"\n",
    "        # Khởi tạo tokenizer và text encoder từ mô hình đã định nghĩa\n",
    "        self.tokenizer = CLIPTokenizer.from_pretrained(CLIP_id)\n",
    "        self.text_encoder = CLIPTextModel.from_pretrained(CLIP_id)\n",
    "        # Chọn thiết bị: sử dụng CUDA nếu có, ngược lại sử dụng CPU\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        # Đóng băng các tham số của text_encoder để không cập nhật trong quá trình huấn luyện\n",
    "        for param in self.text_encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "        # Đặt chế độ eval và chuyển text_encoder sang thiết bị đã chọn\n",
    "        self.text_encoder.eval()\n",
    "        self.text_encoder.to(self.device)\n",
    "\n",
    "    def forward(self, prompts):\n",
    "        \"\"\"\n",
    "        Forward pass của CLIPTextEncoder.\n",
    "        \n",
    "        Tham số:\n",
    "         - prompts: danh sách các chuỗi mô tả ảnh.\n",
    "         \n",
    "        Trả về:\n",
    "         - last_hidden_states: tensor các vector embedding của các prompt.\n",
    "        \"\"\"\n",
    "        inputs = self.tokenizer(\n",
    "            prompts,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.text_encoder.config.max_position_embeddings,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        # Chuyển các tensor về cùng device với text_encoder\n",
    "        input_ids = inputs.input_ids.to(self.device)\n",
    "        attention_mask = inputs.attention_mask.to(self.device)\n",
    "        # Tính toán embedding của văn bản mà không cập nhật gradient\n",
    "        with torch.no_grad():\n",
    "            text_encoder_output = self.text_encoder(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "        # Lấy vector embedding cuối cùng từ text encoder\n",
    "        last_hidden_states = text_encoder_output.last_hidden_state\n",
    "        return last_hidden_states\n",
    "\n",
    "\n",
    "# ------------------------- Pre-trained VAE -------------------------\n",
    "# Sử dụng mô hình Variational Autoencoder (VAE) đã được huấn luyện sẵn từ HuggingFace\n",
    "VAE_id = \"stabilityai/sd-vae-ft-mse\"\n",
    "vae = AutoencoderKL.from_pretrained(VAE_id)\n",
    "# Đóng băng các tham số của VAE để không cập nhật trong quá trình huấn luyện\n",
    "vae.requires_grad_(False)\n",
    "vae.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde9adde",
   "metadata": {},
   "source": [
    "## Khai báo hàm trực quan hóa ảnh và scale giá trị ảnh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473bf08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images(images, title=\"\", titles=[]):\n",
    "    \"\"\"\n",
    "    Hiển thị tập hợp các ảnh trong một lưới (grid) sử dụng matplotlib.\n",
    "    \n",
    "    Tham số:\n",
    "      - images: danh sách các tensor ảnh với định dạng (C, H, W).\n",
    "      - title: tiêu đề chung cho toàn bộ hình ảnh.\n",
    "      - titles: danh sách tiêu đề cho từng ảnh (nếu có).\n",
    "    \"\"\"\n",
    "    # Tạo figure với kích thước 8x8 inch\n",
    "    plt.figure(figsize=(8, 8))\n",
    "\n",
    "    # Lặp qua các ảnh, giới hạn hiển thị tối đa là 25 ảnh\n",
    "    for i in range(min(25, len(images))):\n",
    "        # Tạo subplot trong lưới 5x5, vị trí subplot là i+1 (do chỉ số bắt đầu từ 1)\n",
    "        plt.subplot(5, 5, i + 1)\n",
    "        # Đổi chiều của tensor từ (C, H, W) sang (H, W, C), chuyển về CPU và chuyển đổi sang numpy để hiển thị\n",
    "        img = images[i].permute(1, 2, 0).cpu().numpy()\n",
    "        # Hiển thị ảnh\n",
    "        plt.imshow(img)\n",
    "        # Nếu có tiêu đề riêng cho từng ảnh, hiển thị tiêu đề\n",
    "        if titles:\n",
    "            plt.title(titles[i])\n",
    "        # Tắt hiển thị trục để hình ảnh trông gọn gàng hơn\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "    # Hiển thị tiêu đề chung cho toàn bộ figure\n",
    "    plt.suptitle(title)\n",
    "    # Điều chỉnh bố cục của các subplot sao cho không bị chồng lấn\n",
    "    plt.tight_layout()\n",
    "    # Hiển thị figure\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def rescale(value, in_range, out_range, clamp=False):\n",
    "    \"\"\"\n",
    "    Chuyển đổi giá trị từ khoảng in_range sang khoảng out_range.\n",
    "    \n",
    "    Tham số:\n",
    "      - value: giá trị (hoặc tensor) cần chuyển đổi.\n",
    "      - in_range: tuple (in_min, in_max) xác định khoảng giá trị đầu vào.\n",
    "      - out_range: tuple (out_min, out_max) xác định khoảng giá trị đầu ra.\n",
    "      - clamp: nếu True, giới hạn giá trị kết quả trong khoảng out_range.\n",
    "      \n",
    "    Trả về:\n",
    "      - rescaled_value: giá trị sau khi chuyển đổi về khoảng out_range.\n",
    "    \"\"\"\n",
    "    # Lấy giá trị nhỏ nhất và lớn nhất trong khoảng đầu vào\n",
    "    in_min, in_max = in_range\n",
    "    # Lấy giá trị nhỏ nhất và lớn nhất trong khoảng đầu ra\n",
    "    out_min, out_max = out_range\n",
    "\n",
    "    # Tính khoảng chênh lệch (span) của in_range và out_range\n",
    "    in_span = in_max - in_min\n",
    "    out_span = out_max - out_min\n",
    "\n",
    "    # Chuyển đổi value về tỷ lệ [0, 1]; thêm 1e-8 để tránh chia cho 0\n",
    "    scaled_value = (value - in_min) / (in_span + 1e-8)\n",
    "    # Chuyển đổi giá trị từ [0, 1] về khoảng out_range\n",
    "    rescaled_value = out_min + (scaled_value * out_span)\n",
    "\n",
    "    # Nếu yêu cầu, giới hạn giá trị kết quả trong khoảng out_range\n",
    "    if clamp:\n",
    "        rescaled_value = torch.clamp(rescaled_value, out_min, out_max)\n",
    "\n",
    "    return rescaled_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d8cf4b",
   "metadata": {},
   "source": [
    "## Khai báo class Pytorch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d2dec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Đặt kích thước ảnh mong muốn (chiều rộng và chiều cao)\n",
    "WIDTH, HEIGHT = 32, 32\n",
    "\n",
    "# Đặt batch size cho quá trình huấn luyện\n",
    "batch_size = 32\n",
    "\n",
    "class EmojiDataset(Dataset):\n",
    "    def __init__(self, csv_files, image_folder, transform=None):\n",
    "        \"\"\"\n",
    "        Khởi tạo dataset cho Emoji.\n",
    "        \n",
    "        Tham số:\n",
    "         - csv_files: danh sách đường dẫn các file CSV chứa thông tin metadata.\n",
    "         - image_folder: đường dẫn thư mục chứa ảnh.\n",
    "         - transform: các biến đổi (augmentation) áp dụng lên ảnh nếu có.\n",
    "        \"\"\"\n",
    "        # Đọc và nối tất cả các file CSV thành 1 DataFrame duy nhất\n",
    "        self.dataframe = pd.concat([pd.read_csv(csv_file)\n",
    "                                   for csv_file in csv_files])\n",
    "        self.images_folder = image_folder\n",
    "\n",
    "        # Tạo cột \"image_path\" bằng cách thay thế dấu \"\\\" thành \"/\" trong cột \"file_name\"\n",
    "        self.dataframe[\"image_path\"] = self.dataframe[\"file_name\"].str.replace(\n",
    "            \"\\\\\", \"/\")\n",
    "\n",
    "        # Lấy danh sách đường dẫn ảnh và danh sách tiêu đề (prompt) từ DataFrame\n",
    "        self.image_paths = self.dataframe[\"image_path\"].tolist()\n",
    "        self.titles = self.dataframe[\"prompt\"].tolist()\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        # Trả về tổng số mẫu trong dataset\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Xây dựng đường dẫn đầy đủ đến ảnh bằng cách nối thư mục chứa ảnh với đường dẫn ảnh\n",
    "        image_path = self.images_folder + \"/\" + self.image_paths[idx]\n",
    "        # Lấy tiêu đề tương ứng với ảnh\n",
    "        title = self.titles[idx]\n",
    "        # Loại bỏ các ký tự không mong muốn (dấu ngoặc kép và dấu ngoặc đơn)\n",
    "        title = title.replace('\"', \"\").replace(\"'\", \"\")\n",
    "        # Mở ảnh với PIL và chuyển sang định dạng RGB\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "        # Nếu có áp dụng transform, thì biến đổi ảnh theo các bước đã định nghĩa\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Trả về ảnh đã xử lý và tiêu đề của ảnh đó\n",
    "        return image, title\n",
    "\n",
    "\n",
    "# ------------------ Khởi tạo DataLoader ------------------\n",
    "# Định nghĩa các phép biến đổi (transforms) cho ảnh\n",
    "transform = transforms.Compose([\n",
    "    # Resize ảnh về kích thước (WIDTH, HEIGHT) sử dụng nội suy BICUBIC\n",
    "    transforms.Resize(\n",
    "        (WIDTH, HEIGHT), interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "    transforms.ToTensor(),  # Chuyển ảnh thành tensor\n",
    "    # Chuẩn hóa tensor ảnh với mean và std để giá trị nằm trong khoảng [-1, 1]\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "# Đường dẫn các file CSV chứa metadata (cần thay đổi đường dẫn phù hợp với máy của bạn)\n",
    "csv_files = [\"/content/blobs_crawled_data/metadata.csv\"]\n",
    "# Đường dẫn tới thư mục chứa ảnh\n",
    "image_folder = \"/content/blobs_crawled_data/images\"\n",
    "\n",
    "# Khởi tạo dataset từ class EmojiDataset\n",
    "train_dataset = EmojiDataset(\n",
    "    csv_files=csv_files,\n",
    "    image_folder=image_folder,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# Tạo DataLoader cho quá trình huấn luyện\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,           # Trộn dữ liệu mỗi epoch\n",
    "    num_workers=2,          # Sử dụng 2 worker để tải dữ liệu song song\n",
    "    pin_memory=True,        # Tăng tốc độ chuyển dữ liệu sang GPU nếu có\n",
    "    persistent_workers=True  # Duy trì worker giữa các epoch để giảm thời gian khởi động lại\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AIOEx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
